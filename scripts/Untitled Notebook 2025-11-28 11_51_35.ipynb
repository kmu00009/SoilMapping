{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a33b8c9-4f7e-4f10-a6c9-7a3e2cc33f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from rasterio.merge import merge as rio_merge\n",
    "from pathlib import Path\n",
    "import xgboost as xgb\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dae2dfa-2036-4f14-9f34-9e9293cecb21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n",
    "def split_dataframe(df, chunk_size):\n",
    "    num_chunks = len(df) // chunk_size + int(len(df) % chunk_size != 0)\n",
    "    return [df.iloc[i*chunk_size:(i+1)*chunk_size] for i in range(num_chunks)]\n",
    "\n",
    "def extract_index(filename):\n",
    "    match = re.search(r'class_(\\d+)\\.csv', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "def get_model_feature_names(model_path):\n",
    "    try:\n",
    "        model = joblib.load(model_path)\n",
    "        if hasattr(model, 'feature_names'):\n",
    "            return model.feature_names\n",
    "        elif hasattr(model, 'get_booster'):\n",
    "            return model.get_booster().feature_names\n",
    "        else:\n",
    "            print(\"Model does not have feature_names attribute.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or extracting feature names: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc3657d6-6344-49da-9055-0529c4514023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predictClass(infile, outpath, i, classifier, scaler, feature_names):\n",
    "    import os\n",
    "    try:\n",
    "        # Load data\n",
    "        df = pd.read_csv(infile)\n",
    "        print(f\"Processing chunk {i}\")\n",
    "        df_pred = df[feature_names]\n",
    "        # Use only XGBClassifier for both class and probability prediction\n",
    "        temp_model_file = 'temp_model.json'\n",
    "        classifier.save_model(temp_model_file)\n",
    "        sk_classifier = xgb.XGBClassifier()\n",
    "        try:\n",
    "            sk_classifier.load_model(temp_model_file)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load model into XGBClassifier: {e}\")\n",
    "        # Predict class values\n",
    "        class_values = sk_classifier.predict(df_pred)\n",
    "        # Compute probabilities\n",
    "        prob_values = sk_classifier.predict_proba(df_pred)\n",
    "        confidence_values = np.max(prob_values, axis=1)\n",
    "        confidence_values = np.clip(confidence_values, 0, 1)  # Clip confidence to [0, 1]\n",
    "        print(f\"Chunk {i} confidence range: {confidence_values.min():.6f} to {confidence_values.max():.6f}, dtype: {confidence_values.dtype}\")\n",
    "        print(f\"Unique confidence values (sample): {np.unique(confidence_values)[:10]}\")\n",
    "        # Add predictions to output DataFrame\n",
    "        df['Class'] = class_values + 1  # Adjust to 1-based labels\n",
    "        df['Confidence'] = confidence_values\n",
    "        # Save to CSV with error handling\n",
    "        output_file = os.path.join(outpath, f'class_{i}.csv')\n",
    "        try:\n",
    "            df.to_csv(output_file, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while writing CSV: {e}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if os.path.exists(temp_model_file):\n",
    "            os.remove(temp_model_file)\n",
    "            print(f\"Removed temporary model file: {temp_model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bc033f3-f02a-45ad-8f7d-d6607eba8182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict(grid):\n",
    "    # Load the pre-trained model and scaler\n",
    "    try:\n",
    "        model_path = '/dbfs/mnt/lab/unrestricted/KritiM/classification/model_10_features.joblib'\n",
    "        scaler_path = '/dbfs/mnt/lab/unrestricted/KritiM/classification/scaler.joblib'\n",
    "        best_model = joblib.load(model_path)\n",
    "        scaler = joblib.load(scaler_path)\n",
    "        scaler_feature_names = scaler.feature_names_in_.tolist()\n",
    "        print(f\"Scaler feature names (full numerical): {scaler_feature_names}\")\n",
    "        model_feature_names = get_model_feature_names(model_path)\n",
    "        if model_feature_names is None:\n",
    "            print(\"Falling back to scaler feature names for model.\")\n",
    "            model_feature_names = scaler_feature_names\n",
    "        print(f\"Model feature names (subset): {model_feature_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or scaler: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Define categorical columns\n",
    "    categorical_cols = ['Landcover_LE', 'Profile_depth', 'CaCO3_rank', 'Texture_group', \n",
    "                    'Aggregate_texture', 'Aquifers', 'bedrock_raster_50m', 'ALC_old']\n",
    "    \n",
    "    # Prepare data for classification\n",
    "    dftrain = pd.read_csv('/dbfs/mnt/lab/unrestricted/KritiM/classification/trainingSample.csv')\n",
    "    mode_values = {col: dftrain[col].mode()[0] for col in categorical_cols if col in dftrain.columns}\n",
    "    mean_values = dftrain.select_dtypes(include='number').mean()\n",
    "    median_values = dftrain.select_dtypes(include='number').median()\n",
    "    traincols = dftrain.columns.tolist()\n",
    "    pathtogrids = Path('/dbfs/mnt/lab/unrestricted/KritiM/GRID/')\n",
    "    subdirectories = [subdir for subdir in pathtogrids.iterdir() if subdir.is_dir()]\n",
    "    subdirectories.sort()\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for folder in subdirectories:\n",
    "        print(f'Working on folder: {folder}')\n",
    "        files = [file for file in folder.glob(grid + '*.tif') if file.is_file()]\n",
    "        if not files:\n",
    "            print(f\"No files found for grid {grid} in {folder}\")\n",
    "            continue\n",
    "        for file in files:\n",
    "            grid_name = file.name[:5]\n",
    "            var = file.name[6:-4]\n",
    "            print(f\"Processing file: {file.name}, Grid: {grid_name}, Variable: {var}\")\n",
    "            with rio.open(file, 'r') as src:\n",
    "                data = src.read(1).ravel()\n",
    "                if 'EAST' not in df.columns:\n",
    "                    rows, cols = np.meshgrid(\n",
    "                        np.arange(src.height),\n",
    "                        np.arange(src.width),\n",
    "                        indexing=\"ij\"\n",
    "                    )\n",
    "                    xs, ys = rio.transform.xy(src.transform, rows, cols)\n",
    "                    df['EAST'] = np.array(xs).ravel()\n",
    "                    df['NORTH'] = np.array(ys).ravel()\n",
    "                df[var] = data\n",
    "                \n",
    "    print(f'Created dataframe for {grid}...')\n",
    "    print(\"Dataframe columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Check for missing features\n",
    "    expected_features = scaler_feature_names + [col for col in categorical_cols if col in df.columns]\n",
    "    missing_features = [col for col in expected_features if col not in df.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Error: Missing features for grid {grid}: {missing_features}\")\n",
    "        return\n",
    "    \n",
    "    # Write dataframe to CSV\n",
    "    outdir = Path('/dbfs/mnt/lab/unrestricted/KritiM/Table')\n",
    "    \n",
    "    try:\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create directory {outdir}: {e}\")\n",
    "        raise\n",
    "    df.to_csv(outdir / (grid + '.csv'), index=False)\n",
    "    \n",
    "    # Read one of the raster grid files for profile information\n",
    "    pathraster = Path('/dbfs/mnt/lab/unrestricted/KritiM//GRID/Elevation_dm')\n",
    "    raster = f\"{grid}_Elevation_dm.tif\"\n",
    "    try:\n",
    "        with rio.open(os.path.join(pathraster, raster), 'r') as src:\n",
    "            profile = src.profile\n",
    "            profile.update(count=1)\n",
    "            band = src.read(1)\n",
    "            nodata_value = src.nodatavals[0] if src.nodatavals else -9999\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading raster file: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Read the dataframe for prediction\n",
    "    pathdata = Path('/dbfs/mnt/lab/unrestricted/KritiM/Table')\n",
    "    df = pd.read_csv(pathdata / f\"{grid}.csv\")\n",
    "    df = df.replace(nodata_value, np.nan)\n",
    "    \n",
    "    # Save original NaN mask for later masking in raster\n",
    "    nan_mask = df.isna().any(axis=1)\n",
    "    \n",
    "    # Impute NaNs: numerical with median, categorical with mode\n",
    "    for col in df.columns:\n",
    "        if col in categorical_cols:\n",
    "            if col in mode_values:\n",
    "                df[col] = df[col].fillna(mode_values[col])\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if col in median_values:\n",
    "                df[col] = df[col].fillna(median_values[col])\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "            \n",
    "    # Downcast numerical columns\n",
    "    for col in df.select_dtypes(include='number').columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    print(\"Columns in df (after imputation):\", df.columns.tolist())\n",
    "    \n",
    "    # Scale numerical columns\n",
    "    num_cols = [col for col in scaler_feature_names if col not in categorical_cols]\n",
    "    try:\n",
    "        print('Scaling the prediction dataframe...')\n",
    "        df_num_scaled = pd.DataFrame(\n",
    "            scaler.transform(df[num_cols]),\n",
    "            columns=num_cols, index=df.index\n",
    "        )\n",
    "        df_scaled = pd.concat([df_num_scaled, df[categorical_cols]], axis=1)\n",
    "        print('Scaling completed...')\n",
    "        \n",
    "        # Select only expected features\n",
    "        df_scaled_select = df_scaled[model_feature_names]\n",
    "        print(\"Columns in scaled dataset:\", df_scaled_select.columns.tolist())\n",
    "        print('Shape of dataframe:', df_scaled_select.shape)\n",
    "        \n",
    "        chunk_size = 100000\n",
    "        chunks = split_dataframe(df_scaled_select, chunk_size)\n",
    "        \n",
    "        tmp = Path('/dbfs/mnt/lab/unrestricted/KritiM/Predict') / grid\n",
    "        tmp.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        inFiles = list(tmp.glob('data_*.csv'))\n",
    "        if not inFiles:\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk.to_csv(tmp / f'data_{i}.csv', index=False)\n",
    "            inFiles = list(tmp.glob('data_*.csv'))\n",
    "        \n",
    "        for i, file in enumerate(inFiles):\n",
    "            print(f'Predicting classification for grid {grid}, chunk {i}')\n",
    "            predictClass(file, tmp, i, best_model, scaler, model_feature_names)\n",
    "        \n",
    "        print('Merging the classified data...')\n",
    "        toMergeC = list(tmp.glob('class_*.csv'))\n",
    "        if not toMergeC:\n",
    "            print(f\"No class_*.csv files found for grid {grid}. Skipping merge step.\")\n",
    "            return\n",
    "        \n",
    "        toMergeCF = sorted(toMergeC, key=lambda x: extract_index(str(x)))\n",
    "        dfsC = []\n",
    "        for i in toMergeCF:\n",
    "            print(f\"Reading dataframe {i}\")\n",
    "            df_chunk = pd.read_csv(i)\n",
    "            if 'Confidence' not in df_chunk.columns:\n",
    "                print(f\"Error: 'Confidence' column missing in {i}\")\n",
    "                return\n",
    "            subset = df_chunk[['Class', 'Confidence']]\n",
    "            dfsC.append(subset)\n",
    "        MergedC = pd.concat(dfsC)\n",
    "        \n",
    "        # Create full prediction and confidence arrays\n",
    "        full_class_pred = MergedC['Class'].values\n",
    "        full_confidence_pred = MergedC['Confidence'].values\n",
    "        \n",
    "        # Reshape the 'Class' and 'Confidence' arrays\n",
    "        if len(full_class_pred) != band.size:\n",
    "            raise ValueError(f\"Prediction array length {len(full_class_pred)} does not match raster pixel count {band.size}.\")\n",
    "        S_class = np.reshape(full_class_pred, (band.shape[0], band.shape[1]))\n",
    "        S_confidence = np.reshape(full_confidence_pred, (band.shape[0], band.shape[1]))\n",
    "        \n",
    "        # Apply the original NaN mask as NoData\n",
    "        nan_mask_2d = np.reshape(nan_mask.values, (band.shape[0], band.shape[1]))\n",
    "        S_class[nan_mask_2d] = nodata_value\n",
    "        S_confidence[nan_mask_2d] = nodata_value\n",
    "        \n",
    "        # Clip confidence to [0, 1] (except nodata)\n",
    "        S_confidence = np.where(S_confidence != nodata_value, np.clip(S_confidence, 0, 1), nodata_value)\n",
    "        \n",
    "        # Print debug info before writing\n",
    "        print(f\"S_class shape: {S_class.shape}, dtype: {S_class.dtype}, min: {np.nanmin(S_class)}, max: {np.nanmax(S_class)}\")\n",
    "        print(f\"S_confidence shape: {S_confidence.shape}, dtype: {S_confidence.dtype}, min: {np.nanmin(S_confidence)}, max: {np.nanmax(S_confidence)}\")\n",
    "        print(f\"Unique S_confidence values (sample): {np.unique(S_confidence[~np.isnan(S_confidence)])[:10]}\")\n",
    "        \n",
    "        # Ensure profile has correct nodata value\n",
    "        profile.update(nodata=nodata_value)\n",
    "        out_dir = Path('/dbfs/mnt/lab/unrestricted/KritiM/Predict')\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        local_class_path = f'/tmp/{grid}_predict_xgb.tif'\n",
    "        local_conf_path = f'/tmp/{grid}_confidence_xgb.tif'\n",
    "        dbfs_class_path = out_dir / f'{grid}_predict_xgb.tif'\n",
    "        dbfs_conf_path = out_dir / f'{grid}_confidence_xgb.tif'\n",
    "        try:\n",
    "            with rio.open(local_class_path, 'w', **profile) as dst:\n",
    "                dst.write(S_class.astype(rio.float32), 1)\n",
    "            shutil.move(local_class_path, dbfs_class_path)\n",
    "            print(f\"Moved {local_class_path} to {dbfs_class_path}\")\n",
    "            with rio.open(local_conf_path, 'w', **profile) as dst:\n",
    "                dst.write(S_confidence.astype(rio.float32), 1)\n",
    "            shutil.move(local_conf_path, dbfs_conf_path)\n",
    "            print(f\"Moved {local_conf_path} to {dbfs_conf_path}\")\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Failed to write raster for grid {grid}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return\n",
    "        \n",
    "        # Delete the temporary folder\n",
    "        shutil.rmtree(tmp)\n",
    "    except Exception as e:\n",
    "        print(f'Error predicting for {grid}: {e}')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e8fc08-8c16-4b72-80ab-4fe844b98e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "path = Path('/dbfs/mnt/lab/unrestricted/KritiM/GRID/Elevation_dm/')\n",
    "files = [file for file in folder.glob('*.tif') if file.is_file()]\n",
    "# predict(files[0].name[:5])\n",
    "for file in files:\n",
    "    grid = file.name[:5]\n",
    "    # print(grid)\n",
    "    output_file = Path('/dbfs/mnt/lab/unrestricted/KritiM/Predict') / f'{grid}_predict_xgb.tif'\n",
    "    if not output_file.exists():\n",
    "        predict(grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9b38e63-67ef-4607-8ba3-ab78eab77fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('Merging the grids together...')\n",
    "directory = '/dbfs/mnt/lab/unrestricted/KritiM/Predict'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "# Merge class rasters\n",
    "filenames = glob.glob(os.path.join(directory, '*_predict_xgb.tif'))\n",
    "output_file = '/dbfs/mnt/lab/unrestricted/KritiM/soil_predict_xgb_.tif'\n",
    "command = f\"gdal_merge.py -a_nodata -9999 -o \\\"{output_file}\\\" \" + \" \".join([f\"\\\"{file}\\\"\" for file in filenames])\n",
    "os.system(command)\n",
    "\n",
    "# Merge confidence rasters\n",
    "confidence_filenames = glob.glob(os.path.join(directory, '*_confidence_xgb.tif'))\n",
    "confidence_output_file = 'FinalOutputs/soil_confidence_xgb_.tif'\n",
    "command = f\"gdal_merge.py -a_nodata -9999 -o \\\"{confidence_output_file}\\\" \" + \" \".join([f\"\\\"{file}\\\"\" for file in confidence_filenames])\n",
    "os.system(command)\n",
    "\n",
    "end = time.time()\n",
    "timetaken = convert(end-start)\n",
    "print('Time taken for processing: ', timetaken)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-11-28 11_51_35",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
