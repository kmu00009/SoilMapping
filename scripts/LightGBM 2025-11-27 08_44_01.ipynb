{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e2c5ed-5b86-40ce-9a0a-c5312f66f626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install required packages\n",
    "%pip install --upgrade lightgbm xgboost scikit-learn joblib matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab10c15-5c1b-473f-989b-031f402fa79c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Import libraries and setup\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37cb121c-52c7-46fc-a0bb-ba3dd0cad63b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Utility functions\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "def setup_logging(pathC):\n",
    "    log_file = os.path.join(pathC, 'training.log')\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "    return log_file\n",
    "\n",
    "def get_memory_usage():\n",
    "    import psutil\n",
    "    return f\"{psutil.virtual_memory().used / 1e9:.2f} GB\"\n",
    "\n",
    "def convert(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{int(h)}h {int(m)}m {int(s)}s\"\n",
    "\n",
    "def compute_class_weights(y):\n",
    "    from sklearn.utils.class_weight import compute_sample_weight\n",
    "    return compute_sample_weight('balanced', y)\n",
    "\n",
    "def split_data(df):\n",
    "    train, temp = train_test_split(df, test_size=0.3, stratify=df['target'], random_state=42)\n",
    "    validate, test = train_test_split(temp, test_size=0.5, stratify=temp['target'], random_state=42)\n",
    "    return train, validate, test\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_valid, y_valid, X_test, y_test, pathC, model_name, report_name, selected_features, selected_importances):\n",
    "    # Cross-validation for accuracy and f1 (no callbacks, fresh model each fold)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    base_estimator = lgb.LGBMClassifier(\n",
    "        random_state=42,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        n_estimators=300\n",
    "    )\n",
    "    acc_scores = cross_val_score(base_estimator, X_train, y_train, cv=skf, scoring='accuracy', n_jobs=1)\n",
    "    f1_scores = cross_val_score(base_estimator, X_train, y_train, cv=skf, scoring='f1_weighted', n_jobs=1)\n",
    "    # Final model fit with early stopping\n",
    "    model = lgb.LGBMClassifier(\n",
    "        random_state=42,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        n_estimators=1000\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric='multi_logloss',\n",
    "        callbacks=[lgb.early_stopping(10)]\n",
    "    )\n",
    "    # Evaluate on train, valid, test\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    # Print confusion matrices and classification reports\n",
    "    print(\"\\nTraining Confusion Matrix:\")\n",
    "    train_cm = confusion_matrix(y_train, y_train_pred)\n",
    "    print(train_cm)\n",
    "    train_cr = classification_report(y_train, y_train_pred)\n",
    "    print(\"Training Classification Report:\")\n",
    "    print(train_cr)\n",
    "    print(\"\\nValidation Confusion Matrix:\")\n",
    "    valid_cm = confusion_matrix(y_valid, y_valid_pred)\n",
    "    print(valid_cm)\n",
    "    valid_cr = classification_report(y_valid, y_valid_pred)\n",
    "    print(\"Validation Classification Report:\")\n",
    "    print(valid_cr)\n",
    "    print(\"\\nTesting Confusion Matrix:\")\n",
    "    test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(test_cm)\n",
    "    test_cr = classification_report(y_test, y_test_pred)\n",
    "    print(\"Testing Classification Report:\")\n",
    "    print(test_cr)\n",
    "    metrics = {\n",
    "        'cv_train_accuracy_mean': np.mean(acc_scores),\n",
    "        'cv_train_accuracy_std': np.std(acc_scores),\n",
    "        'cv_train_f1_mean': np.mean(f1_scores),\n",
    "        'cv_train_f1_std': np.std(f1_scores),\n",
    "        'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'valid_accuracy': accuracy_score(y_valid, y_valid_pred),\n",
    "        'test_accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'train_f1': f1_score(y_train, y_train_pred, average='weighted'),\n",
    "        'valid_f1': f1_score(y_valid, y_valid_pred, average='weighted'),\n",
    "        'test_f1': f1_score(y_test, y_test_pred, average='weighted')\n",
    "    }\n",
    "    report = {\n",
    "        'features': list(selected_features),\n",
    "        'importances': list(selected_importances),\n",
    "        **metrics\n",
    "    }\n",
    "    joblib.dump(model, os.path.join(pathC, model_name))\n",
    "    # Write metrics, confusion matrices, and classification reports to the report file\n",
    "    with open(os.path.join(pathC, report_name), 'w') as f:\n",
    "        f.write(\"Model Report\\n\")\n",
    "        f.write(\"==============\\n\")\n",
    "        for k, v in report.items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "        f.write(\"\\nTraining Confusion Matrix:\\n\")\n",
    "        f.write(str(train_cm) + \"\\n\")\n",
    "        f.write(\"Training Classification Report:\\n\")\n",
    "        f.write(train_cr + \"\\n\")\n",
    "        f.write(\"\\nValidation Confusion Matrix:\\n\")\n",
    "        f.write(str(valid_cm) + \"\\n\")\n",
    "        f.write(\"Validation Classification Report:\\n\")\n",
    "        f.write(valid_cr + \"\\n\")\n",
    "        f.write(\"\\nTesting Confusion Matrix:\\n\")\n",
    "        f.write(str(test_cm) + \"\\n\")\n",
    "        f.write(\"Testing Classification Report:\\n\")\n",
    "        f.write(test_cr + \"\\n\")\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd3e8f6-3383-4b61-b3c9-ee2d4cb1aced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    start = time.time()\n",
    "    pathC = os.environ.get('CLASSIFICATION_DIR', '/dbfs/mnt/lab/unrestricted/KritiM/classification/')\n",
    "    training_file = os.environ.get('TRAINING_FILE', os.path.join(pathC, 'trainingSample.csv'))\n",
    "    os.makedirs(pathC, exist_ok=True)\n",
    "    log_file = setup_logging(pathC)\n",
    "    logging.info(f\"Starting training script - Output directory: {pathC}\")\n",
    "    import sklearn\n",
    "    logging.info(f\"Python packages: lightgbm {lgb.__version__}, sklearn {sklearn.__version__}, pandas {pd.__version__}\")\n",
    "    logging.info(f\"Initial memory usage: {get_memory_usage()}\")\n",
    "    logging.info('Loading the labelled data...')\n",
    "    try:\n",
    "        df = pd.read_csv(training_file)\n",
    "        logging.info(f\"Successfully loaded data from {training_file}\")\n",
    "        logging.info(f\"Initial dataframe shape: {df.shape}\")\n",
    "        logging.info(f\"Memory usage after load: {get_memory_usage()}\")\n",
    "        df = df.drop_duplicates()\n",
    "        df = df.dropna()\n",
    "        logging.info(f\"Shape after cleaning: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading/cleaning data: {str(e)}\")\n",
    "        raise\n",
    "    df['target'] = df['target'].astype(int) - 1\n",
    "    print(\"Class distribution:\")\n",
    "    print(df['target'].value_counts(normalize=True))\n",
    "    print('assign categorical and numerical columns...')\n",
    "    categorical_cols = ['Landcover_LE', 'Profile_depth', 'CaCO3_rank', 'Texture_group', \n",
    "                        'Aggregate_texture', 'Aquifers', 'bedrock_raster_50m', 'ALC_old']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "    for col in df.select_dtypes(include='number').columns:\n",
    "        if col != 'target':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    print(f\"Shape of training data: {df.shape}\")\n",
    "    print(f\"Unique target values: {np.unique(df['target'])}\")\n",
    "    train, validate, test = split_data(df)\n",
    "    X_train = train.drop('target', axis=1)\n",
    "    y_train = train['target']\n",
    "    X_valid = validate.drop('target', axis=1)\n",
    "    y_validate = validate['target']\n",
    "    X_test = test.drop('target', axis=1)\n",
    "    y_test = test['target']\n",
    "    num_cols = [col for col in X_train.columns if col not in categorical_cols]\n",
    "    scaler = StandardScaler()\n",
    "    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "    X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "    joblib.dump(scaler, os.path.join(pathC, 'scaler.joblib'))\n",
    "    # --- PERMUTATION FEATURE IMPORTANCE FEATURE SELECTION ---\n",
    "    initial_model = xgb.XGBClassifier(\n",
    "        random_state=42, enable_categorical=True, objective='multi:softmax',\n",
    "        num_class=len(np.unique(y_train)), eval_metric='mlogloss', tree_method='hist'\n",
    "    )\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 5]\n",
    "    }\n",
    "    # Set n_jobs=1 to avoid BrokenProcessPool error on Databricks/Azure\n",
    "    random_search = RandomizedSearchCV(\n",
    "        initial_model, param_grid, n_iter=20, cv=3, scoring='f1_weighted',\n",
    "        n_jobs=1, random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train, sample_weight=compute_class_weights(y_train))\n",
    "    initial_model = random_search.best_estimator_\n",
    "    perm_importance = permutation_importance(initial_model, X_valid, y_validate, n_repeats=3, random_state=42, n_jobs=1)\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': perm_importance.importances_mean\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    feature_counts = range(3, 15, 1)\n",
    "    results = []\n",
    "    metrics_results = []  # For metrics_vs_features.csv\n",
    "    for n in feature_counts:\n",
    "        print(f\"\\nTraining with top {n} features...\")\n",
    "        selected_features = feature_importance_df['Feature'].head(n).values\n",
    "        selected_importances = feature_importance_df['Importance'].head(n).values\n",
    "        X_train_n = X_train[selected_features]\n",
    "        X_valid_n = X_valid[selected_features]\n",
    "        X_test_n = X_test[selected_features]\n",
    "        model_name = f'model_{n}_features.joblib'\n",
    "        report_name = f'report_{n}_features.txt'\n",
    "        print(f\"\\nTraining and evaluating with top {n} features: {list(selected_features)}\")\n",
    "        res = train_and_evaluate(\n",
    "            X_train_n, y_train, X_valid_n, y_validate, X_test_n, y_test, pathC,\n",
    "            model_name, report_name, selected_features, selected_importances\n",
    "        )\n",
    "        results.append(res)\n",
    "        metrics_results.append({\n",
    "            'num_features': n,\n",
    "            'train_accuracy': res['train_accuracy'],\n",
    "            'valid_accuracy': res['valid_accuracy'],\n",
    "            'test_accuracy': res['test_accuracy'],\n",
    "            'train_f1': res['train_f1'],\n",
    "            'valid_f1': res['valid_f1'],\n",
    "            'test_f1': res['test_f1']\n",
    "        })\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(pathC, 'feature_selection_results.csv'), index=False)\n",
    "    print(f\"\\nResults table saved to {os.path.join(pathC, 'feature_selection_results.csv')}\")\n",
    "    # Save metrics_vs_features.csv\n",
    "    metrics_df = pd.DataFrame(metrics_results)\n",
    "    metrics_csv_path = os.path.join(pathC, 'metrics_vs_features.csv')\n",
    "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "    print(f\"Metrics table saved to {metrics_csv_path}\")\n",
    "    # Plot accuracy vs. number of features\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(metrics_df['num_features'], metrics_df['train_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(metrics_df['num_features'], metrics_df['valid_accuracy'], label='Validation Accuracy')\n",
    "    plt.plot(metrics_df['num_features'], metrics_df['test_accuracy'], label='Test Accuracy')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs. Number of Features')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plot_path = os.path.join(pathC, 'accuracy_vs_features.png')\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Accuracy plot saved to {plot_path}\")\n",
    "    end = time.time()\n",
    "    time_taken = convert(end-start)\n",
    "    logging.info(f\"Total processing time: {time_taken}\")\n",
    "    logging.info(f\"Final memory usage: {get_memory_usage()}\")\n",
    "    logging.info(f\"Log file saved to: {log_file}\")\n",
    "    print(f\"\\nScript completed successfully. Check the log file at: {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LightGBM 2025-11-27 08_44_01",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
