{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "076e333f-460c-4b23-a566-b26d32c027fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# For Spark DataFrame API, use 'dbfs:/...' (no extra '/dbfs' prefix)\n",
    "pathC = os.environ.get('CLASSIFICATION_DIR', 'dbfs:/mnt/lab/unrestricted/KritiM/classification/')\n",
    "training_file = os.environ.get('TRAINING_FILE', f\"{pathC.rstrip('/')}/trainingSample.csv\")\n",
    "\n",
    "# Check if the file exists in DBFS\n",
    "try:\n",
    "    dbutils.fs.ls(training_file)\n",
    "except Exception as e:\n",
    "    raise FileNotFoundError(f\"File not found at {training_file}. Please check the path and ensure the file exists.\")\n",
    "\n",
    "# Read the CSV using Spark\n",
    "# Note: For pandas or local file APIs, use '/dbfs/mnt/...' instead\n",
    "\n",
    "df = spark.read.csv(training_file, header=True, inferSchema=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af0a4c82-6921-4dd8-bc40-24be8513c23e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Convert Spark DataFrame to pandas DataFrame\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# 1) Display schema and check for missing values\n",
    "print(pdf.info())\n",
    "print(\"Missing values per column:\\n\", pdf.isnull().sum())\n",
    "\n",
    "# 2) Handle missing values (drop rows with any missing values for simplicity)\n",
    "pdf = pdf.dropna()\n",
    "\n",
    "# 3) Define categorical and numerical columns\n",
    "categorical_cols = [\n",
    "    'Landcover_LE', 'Profile_depth', 'CaCO3_rank', 'Texture_group', \n",
    "    'Aggregate_texture', 'Aquifers', 'bedrock_raster_50m', 'ALC_old'\n",
    "]\n",
    "categorical_cols = [col for col in categorical_cols if col in pdf.columns]\n",
    "\n",
    "# --- Robust label column detection ---\n",
    "possible_label_names = ['label', 'target', 'class', 'Label', 'Target', 'Class']\n",
    "label_col = None\n",
    "for col in possible_label_names:\n",
    "    if col in pdf.columns:\n",
    "        label_col = col\n",
    "        print(f\"Using '{label_col}' as the label column.\")\n",
    "        break\n",
    "if label_col is None:\n",
    "    # Try last column as a fallback\n",
    "    label_col = pdf.columns[-1]\n",
    "    print(f\"No standard label column found. Using last column '{label_col}' as the label column.\")\n",
    "    print(f\"All columns: {list(pdf.columns)}\")\n",
    "    print(\"If this is not correct, please update 'label_col' in the code.\")\n",
    "\n",
    "# Numerical columns: all non-object, non-categorical, and not the label\n",
    "numerical_cols = [\n",
    "    col for col in pdf.select_dtypes(include=[np.number]).columns \n",
    "    if col not in categorical_cols and col != label_col\n",
    "]\n",
    "\n",
    "# 4) Split into features and label\n",
    "X = pdf.drop(label_col, axis=1)\n",
    "y = pdf[label_col]\n",
    "\n",
    "# 5) Split into train/validation/test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# --- Label encoding: ensure labels are zero-based and contiguous ---\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train.tolist() + y_val.tolist() + y_test.tolist())\n",
    "y_train = le.transform(y_train)\n",
    "y_val = le.transform(y_val)\n",
    "y_test = le.transform(y_test)\n",
    "print(\"Label encoding mapping:\")\n",
    "for idx, class_ in enumerate(le.classes_):\n",
    "    print(f\"{class_} -> {idx}\")\n",
    "\n",
    "# 6) Scale numerical features and encode categorical features AFTER splitting\n",
    "scaler = StandardScaler()\n",
    "if numerical_cols:\n",
    "    X_train_num = scaler.fit_transform(X_train[numerical_cols])\n",
    "    X_val_num = scaler.transform(X_val[numerical_cols])\n",
    "    X_test_num = scaler.transform(X_test[numerical_cols])\n",
    "else:\n",
    "    X_train_num = np.empty((X_train.shape[0], 0))\n",
    "    X_val_num = np.empty((X_val.shape[0], 0))\n",
    "    X_test_num = np.empty((X_test.shape[0], 0))\n",
    "\n",
    "if categorical_cols:\n",
    "    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "    X_train_cat = encoder.fit_transform(X_train[categorical_cols])\n",
    "    X_val_cat = encoder.transform(X_val[categorical_cols])\n",
    "    X_test_cat = encoder.transform(X_test[categorical_cols])\n",
    "else:\n",
    "    X_train_cat = np.empty((X_train.shape[0], 0))\n",
    "    X_val_cat = np.empty((X_val.shape[0], 0))\n",
    "    X_test_cat = np.empty((X_test.shape[0], 0))\n",
    "\n",
    "# Concatenate numerical and categorical features\n",
    "X_train_final = np.hstack([X_train_num, X_train_cat])\n",
    "X_val_final = np.hstack([X_val_num, X_val_cat])\n",
    "X_test_final = np.hstack([X_test_num, X_test_cat])\n",
    "\n",
    "# 7) Show basic stats and confirm shapes match\n",
    "def print_shapes_and_counts(X, y, name):\n",
    "    print(f\"{name} shape: {X.shape}, y shape: {y.shape}\")\n",
    "    print(f\"Label distribution in {name}:\")\n",
    "    print(pd.Series(y).value_counts())\n",
    "\n",
    "print_shapes_and_counts(X_train_final, y_train, \"Train\")\n",
    "print_shapes_and_counts(X_val_final, y_val, \"Validation\")\n",
    "print_shapes_and_counts(X_test_final, y_test, \"Test\")\n",
    "\n",
    "# Assert shapes match for model training\n",
    "assert X_train_final.shape[0] == len(y_train), f\"Train features/labels mismatch: {X_train_final.shape[0]} vs {len(y_train)}\"\n",
    "assert X_val_final.shape[0] == len(y_val), f\"Validation features/labels mismatch: {X_val_final.shape[0]} vs {len(y_val)}\"\n",
    "assert X_test_final.shape[0] == len(y_test), f\"Test features/labels mismatch: {X_test_final.shape[0]} vs {len(y_test)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b663111e-2cfd-4119-87b1-22dfaef2a82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, callbacks\n",
    "\n",
    "# Use preprocessed data from Cell 2\n",
    "# X_train_final, X_val_final, X_test_final, y_train, y_val, y_test must be defined in previous cell\n",
    "\n",
    "# Determine number of features and classes\n",
    "def get_num_classes(y):\n",
    "    unique = np.unique(y)\n",
    "    return len(unique)\n",
    "\n",
    "num_features = X_train_final.shape[1]\n",
    "num_classes = get_num_classes(y_train)\n",
    "\n",
    "# Print class distribution\n",
    "print(\"Class distribution in training set:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u}: {c} samples\")\n",
    "\n",
    "# Compute class weights if imbalanced\n",
    "if np.max(counts) / np.min(counts) > 1.5:\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "    print(\"Using class weights:\", class_weight_dict)\n",
    "else:\n",
    "    class_weight_dict = None\n",
    "    print(\"Class distribution is reasonably balanced; not using class weights.\")\n",
    "\n",
    "# Build a deeper ANN with less regularization\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(num_features,),\n",
    "                 kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Early stopping and learning rate scheduler\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True\n",
    ")\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_final, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_final, y_val),\n",
    "    callbacks=[early_stop, lr_scheduler],\n",
    "    verbose=2,\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_accuracy = model.evaluate(X_train_final, y_train, verbose=0)\n",
    "val_loss, val_accuracy = model.evaluate(X_val_final, y_val, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test_final, y_test, verbose=0)\n",
    "\n",
    "# Calculate F1 scores\n",
    "train_pred = model.predict(X_train_final)\n",
    "val_pred = model.predict(X_val_final)\n",
    "test_pred = model.predict(X_test_final)\n",
    "\n",
    "if num_classes > 2:\n",
    "    train_pred_classes = np.argmax(train_pred, axis=1)\n",
    "    val_pred_classes = np.argmax(val_pred, axis=1)\n",
    "    test_pred_classes = np.argmax(test_pred, axis=1)\n",
    "else:\n",
    "    train_pred_classes = (train_pred > 0.5).astype(int).flatten()\n",
    "    val_pred_classes = (val_pred > 0.5).astype(int).flatten()\n",
    "    test_pred_classes = (test_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "train_f1 = f1_score(y_train, train_pred_classes, average='weighted')\n",
    "val_f1 = f1_score(y_val, val_pred_classes, average='weighted')\n",
    "test_f1 = f1_score(y_test, test_pred_classes, average='weighted')\n",
    "\n",
    "# Print accuracy and F1-score\n",
    "print(f'Train Accuracy: {train_accuracy:.4f}, Train F1 Score: {train_f1:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}')\n",
    "\n",
    "# Plot training/validation loss and accuracy curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ANN 2025-11-26 10_01_50",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
